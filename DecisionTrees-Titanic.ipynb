{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries and load the dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys                             # Read system parameters.\n",
    "import os                              # Interact with the operating system.\n",
    "import numpy as np                     # Work with multi-dimensional arrays and matrices.\n",
    "import pandas as pd                    # Manipulate and analyze data.\n",
    "import matplotlib                      # Create 2D charts.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb                   # Perform data visualization.\n",
    "import scipy as sp                     # Perform scientific computing and advanced mathematics.\n",
    "import sklearn                         # Perform data mining and analysis.\n",
    "from time import time                  # Calculate training time.\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- pandas {}'.format(pd.__version__))\n",
    "print('- Matplotlib {}'.format(matplotlib.__version__))\n",
    "print('- Seaborn {}'.format(sb.__version__))\n",
    "print('- SciPy {}'.format(sp.__version__))\n",
    "print('- scikit-learn {}\\n'.format(sklearn.__version__))\n",
    "\n",
    "# Load the dataset.\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"titanic_data\")\n",
    "print('Data files in this project:', os.listdir(DATA_PATH))\n",
    "data_raw_file = os.path.join(DATA_PATH, 'train.csv')\n",
    "data_raw = pd.read_csv(data_raw_file)\n",
    "print('Loaded {} records from {}.'.format(len(data_raw), data_raw_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and test sets already exist.\n",
    "# A validation set will be split off from the training sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'Survived' is the dependent variable (value to be predicted), so it will be\n",
    "# removed from the training data and put into a separate DataFrame for labels.\n",
    "label_columns = ['Survived']\n",
    "\n",
    "training_columns = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "# Split the training and validation datasets and their labels.\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_raw[training_columns],\n",
    "                                                                            data_raw[label_columns],\n",
    "                                                                            random_state = 1912)\n",
    "\n",
    "print('The training and validation datasets and labels have been split.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for categorical features that need to be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform common preparation on the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform common cleaning and feature engineering tasks on datasets.\n",
    "def prep_dataset(dataset):\n",
    "    \n",
    "    # PROVIDE MISSING VALUES\n",
    "    \n",
    "    # Fill missing Age values with the median age.\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    # Fill missing Fare values with the median fare.\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "\n",
    "    # Fill missing Embarked values with the mode.\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    \n",
    "    # ONE-HOT ENCODING\n",
    "    \n",
    "    cols = ['Pclass', 'Sex', 'Embarked']\n",
    "    \n",
    "    for i in cols:\n",
    "        dummies = pd.get_dummies(dataset[i], prefix = i, drop_first = False)\n",
    "        dataset = pd.concat([dataset, dummies], axis = 1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "X_train = prep_dataset(X_train.copy())\n",
    "\n",
    "X_val = prep_dataset(X_val.copy())\n",
    "\n",
    "print('The dataset has been cleaned and prepared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns that won't be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns from datasets.\n",
    "def drop_unused(dataset):\n",
    "        \n",
    "    dataset = dataset.drop(['PassengerId'], axis = 1)\n",
    "    dataset = dataset.drop(['Cabin'], axis = 1)\n",
    "    dataset = dataset.drop(['Ticket'], axis = 1)\n",
    "    dataset = dataset.drop(['Name'], axis = 1)\n",
    "\n",
    "    # These have been replaced with one-hot encoding.\n",
    "    dataset = dataset.drop(['Pclass'], axis = 1)\n",
    "    dataset = dataset.drop(['Sex'], axis = 1)\n",
    "    dataset = dataset.drop(['Embarked'], axis = 1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "X_train = drop_unused(X_train.copy())\n",
    "\n",
    "X_val = drop_unused(X_val.copy())\n",
    "\n",
    "print('Columns that will not be used for training have been dropped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview current training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a basic decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state = 1912)\n",
    "start = time()\n",
    "tree.fit(X_train, np.ravel(y_train))\n",
    "end = time()\n",
    "train_time = (end - start) * 1000\n",
    "\n",
    "prediction = tree.predict(X_val)\n",
    "\n",
    "# Score using the validation data.\n",
    "score = tree.score(X_val, y_val)\n",
    "\n",
    "print('Decision tree model took {:.2f} milliseconds to fit.'.format(train_time))\n",
    "print('Accuracy: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the decision tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image, display \n",
    "import pydotplus as pdotp\n",
    "\n",
    "def plot_tree(model, image):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(model, out_file = dot_data, \n",
    "                    filled = True,\n",
    "                    rounded = True,\n",
    "                    special_characters = True, \n",
    "                    feature_names = X_train.columns.values.tolist(),\n",
    "                    class_names = ['0', '1'])\n",
    "\n",
    "    graph = pdotp.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png(image)\n",
    "    Image(graph.create_png())\n",
    "    \n",
    "print('A function to plot the decision tree structure has been defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute accuracy, precision, recall, and F<sub>1</sub> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def model_scores(y, prediction):\n",
    "    acc = accuracy_score(y, prediction)\n",
    "    print('Accuracy: {:.0f}%'.format(np.round(acc * 100)))\n",
    "    \n",
    "    precision = precision_score(y, prediction)\n",
    "    print('Precision: {:.0f}%'.format(np.round(precision * 100)))\n",
    "    \n",
    "    recall = recall_score(y, prediction)\n",
    "    print('Recall: {:.0f}%'.format(np.round(recall * 100)))\n",
    "    \n",
    "    f1 = f1_score(y, prediction)\n",
    "    print('F1: {:.0f}%'.format(np.round(f1 * 100)))\n",
    "    \n",
    "print('A function to compute the model scores has been defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a ROC curve and compute the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc(y, prediction_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y, prediction_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr);\n",
    "    plt.xlim([0.0, 1.0]);\n",
    "    plt.ylim([0.0, 1.0]);\n",
    "    plt.title('ROC Curve');\n",
    "    plt.xlabel('False Positive Rate');\n",
    "    plt.ylabel('True Positive Rate');\n",
    "    plt.grid(True);\n",
    "    \n",
    "    auc = roc_auc_score(y, prediction_proba)\n",
    "    print('Area Under Curve: {:.2f}'.format(auc))\n",
    "    \n",
    "print('A function to generate the ROC curve and compute AUC has been defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a precision–recall curve and compute the average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def prc(y, prediction_proba):\n",
    "    precision, recall, thresholds = precision_recall_curve(y, prediction_proba)\n",
    "    \n",
    "    plt.plot(recall, precision);\n",
    "    plt.xlim([0.0, 1.0]);\n",
    "    plt.ylim([0.0, 1.0]);\n",
    "    plt.title('Precision–Recall Curve');\n",
    "    plt.xlabel('Recall');\n",
    "    plt.ylabel('Precision');\n",
    "    plt.grid(True);\n",
    "    \n",
    "    ap = average_precision_score(y, prediction_proba)\n",
    "    print('Average Precision: {:.2f}'.format(ap))\n",
    "    \n",
    "print('A function to generate the PRC and compute average precision has been defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the initial decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required library.\n",
    "!conda install --yes graphviz==2.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(tree, 'titanic.png')\n",
    "display(Image('titanic.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_predict = tree.predict(X_val)\n",
    "\n",
    "model_scores(y_val, initial_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_predict_proba = tree.predict_proba(X_val)\n",
    "\n",
    "roc(y_val, initial_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc(y_val, initial_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing pre-pruning on the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 4, random_state = 1912)\n",
    "start = time()\n",
    "pruned_tree.fit(X_train, np.ravel(y_train))\n",
    "end = time()\n",
    "train_time = (end - start) * 1000\n",
    "\n",
    "prediction = pruned_tree.predict(X_val)\n",
    "\n",
    "# Score using the validation data.\n",
    "score = pruned_tree.score(X_val, y_val)\n",
    "\n",
    "print('Decision tree model took {:.2f} milliseconds to fit.'.format(train_time))\n",
    "print('Accuracy: {:.0f}%'.format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the pruned decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(pruned_tree, 'titanic_pruned.png')\n",
    "display(Image('titanic_pruned.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_predict = pruned_tree.predict(X_val)\n",
    "\n",
    "model_scores(y_val, pruned_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_predict_proba = pruned_tree.predict_proba(X_val)\n",
    "\n",
    "roc(y_val, pruned_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc(y_val, pruned_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a decision tree model using randomized search with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "dist = {'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': sp_randint(2, 10),\n",
    "        'min_samples_split': sp_randint(5, 100),\n",
    "        'min_samples_leaf': sp_randint(5, 100)}\n",
    "\n",
    "search = RandomizedSearchCV(tree, param_distributions = dist, n_iter = 500, \n",
    "                            scoring = 'f1', cv = 5, iid = False, random_state = 1912)\n",
    "search.fit(X_train, np.ravel(y_train));\n",
    "optimized_tree = search.best_estimator_\n",
    "\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(optimized_tree, 'titanic_optimized.png')\n",
    "display(Image('titanic_optimized.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predict = optimized_tree.predict(X_val)\n",
    "\n",
    "model_scores(y_val, optimized_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predict_proba = optimized_tree.predict_proba(X_val)\n",
    "\n",
    "roc(y_val, optimized_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc(y_val, optimized_predict_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
